# 컨트롤러 리소스를 이용한 애플리케이션의 스케일링

## k8s의 스케일링 방법
k8s에서 파드의 관리는 디플로이먼트와 같은 컨트롤러 리소스가 진행한다고 배웠다.
디플로이먼트이 직접 관리하지 않고 레플리카셋(ReplicaSet)이 직접 관리하는 역할을 맡는다.
레플리카셋도 매니페스트 파일로 정의해서 배치할 수 있다.
```yaml
apiVersion: apps/v1 # api는 apps/v1 사용
kind: ReplicaSet # 레플리카셋 정의
metadata:
  name: whoami-web
  labels:
    kiamol: ch06
spec:
  replicas: 1 # 생성할 레플리카의 개수
  selector:   # 관리 대상 파드를 찾기 위한 셀렉터
    matchLabels:
      app: whoami-web
  template: # 파드의 정의가 이 뒤로 이어짐
```
디플로이먼트 정의와의 차이점은 리소스 유형과 파드 수를 적은 replicas 필드가 있다는 것이다.
디플로이먼트와 마찬가지로 레플리카셋을 생성 후, 내부 파드를 제거하면 레플리카셋이 자동으로 파드를 다시 생성해준다.
레플리카셋은 항상 제어 루프를 돌며 관리 중인 리소스 수와 필요한 리소스 수를 확인하기 때문에, 즉각 삭제된 파드를 대체할 수 있다.
**애플리케이션 스케일링**의 경우에도 레플리카 수를 변경하면, 제어 루프 중에 파드 수가 부족한 것을 확인하고 템플릿에서 새로운 파드를 생성한다.

* 스케일링 과정에서 다음과 같은 2가지 의문이 생긴다.
### 1. 스케일링이 빠르게 적용되는 이유는?
실습 환경은 단일 노드 클러스터이기 때문이다.
모든 파드가 하나의 노드에서 실행되는데 이 노드에는 이미 애플리케이션의 도커 이미지가 있다.
운영 클러스터에서 스케일링을 지시하면, 이미지가 없는 노드에서 파드가 실행될 가능성이 높다.
이미지를 먼저 내려 받아야 파드를 실행할 수 있기 때문에, 운영 환경에서는 스케일링이 느릴 수도 있다.

### 2. 응답이 여러 파드에서 오는 이유는?
서비스와 파드가 레이블을 통해 느슨하게 결합돼있기 때문이다. 레플리카셋에서 레플리카의 수를 늘리면, 서비스의 레이블 셀렉터와 일치하는 파드 수도 늘어난다.
레플리카셋에 정의된 파드가 서비스의 엔드포인트로 추가되고, k8s는 서비스에 등록된 파드에 요청을 적절히 분배한다.

## 디플로이먼트와 레플리카셋을 이용한 부하 스케일링
디플로이먼트는 레플리카셋 위에 유용한 관리 계층을 추가한다. 디플로이먼트 정의에 replicas 필드를 추가해 스케일링을 적용할 수 있다.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pi-web
  labels:
    kiamol: ch06
spec:
  replicas: 2 # 필수x, 생략한 경우 1로 설정됨
  selector:
    matchLabels:
      app: pi-web
  template: # 이 이후로 파드의 정의가 이어진다.
```

디플로이먼트를 스케일링하면 기존 레플리카셋의 레플리카 수가 변경된다. 
새 파드의 정의가 포함된 디플로이먼트를 배치하면, 기존 레플리카셋의 파드 수는 0이 되고, 대체하는 레플리카셋이 생성된다.
컨트롤러 리소스에 스케일링을 곧바로 지시하려면 `kubectl scale` 명령을 사용할 수 있다.
하지만 매니페스트 파일로 사용하는 것이 관리 측면에서 안전하다.
예를 들어, 애플리케이션의 성능 문제로 자동 재배치에 오랜 시간이 걸린다면 즉시 `kubectl scale` 명령어로 대응하는 것이 나을 수 있다.
물론 매니페스트 파일도 맞춰 수정해야 한다.

### 레플리카셋 재활용
디플로이먼트가 생성한 파드의 이름은 무작위가 아닌, 디플로이먼트 정의에 포함된 템플릿 속 파드 정의의 해시 값이다.
디플로이먼트의 이전 정의와 일치하는 변경이 적용될 경우 레플리카셋의 이름이 기존 (레플리카 수가 0이 된 채로 남아 있는) 레플리카셋과 같으므로, 이 레플리카셋의 레플리카 수를 증가시켜 변경 사항을 적용한다.

### 프록시 파드
실습으로 한 애플리케이션은 클러스터IP 서비스만 정의되어있다.
외부 노출을 위해 로드밸런서 서비스를 사용하는 프록시 파드를 배치시킬 수 있다.
이 때 웹 컴포넌트와 프록시 모두에 스케일링하면서, 웹 컴포넌트 - 프록시 - 애플리케이션 파드 간 로드밸런싱을 수행할 수 있는 방법이 있다.
프록시 파드를 배치하고 웹에 접속할 때 응답 헤더를 보면 응답을 보내는 프록시 파드가 변경된다.
이 구조에서는 캐시에 사용하는 볼륨이 파드마다 따로 갖고 있어 응답이 캐시되지 않은 파드가 요청을 받을 수 있다는 문제점이 있다.
이럴 떄 스테이트풀셋의 데몬셋이 등장한다.

### 데몬셋
> 데몬셋이란?
리눅스 백그라운드에서 단일 인스턴스로 동작하면서 시스템 관련 기능을 제공하는 프로세스

k8s에서의 데몬셋은 클러스터 내 모든 노드 또는 셀렉터와 일치하는 일부 노드에서 단일 레플리카 또는 파드로 동작하는 리소스다.

데몬셋은 보통 각 노드에서 정보를 수집해 중앙의 수집 모듈에 전달하는 인프라 수준의 관심사와 관련된 목적으로 자주 사용된다.
각 노드마다 파드가 하나씩 동작하면서 해당 노드의 데이터를 수집하는 역할을 수행한다.
좋은 예시로 리버스 프록시가 있다.
nginx 파드 하나로 수천 개의 동시 요청을 처리할 수 있다. 고로, 파드를 여러 개 둘 필요도 없고 한 노드에 하나씩 배치되는 것만 보장하면 된다.
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: pi-proxy
  labels:
    kiamol: ch06
spec:
  selector:
    matchLabels:
      app: pi-proxy
  template:
    metadata:
      labels:
        app: pi-proxy
    spec: # 이 뒤로 파드 정의가 이어진다.
```

이 실습 환경에서는 단일 노드 클러스터이므로 데몬셋의 파드도 하나 뿐이지만, 노드가 늘어나면 노드 개수만큼 파드가 늘어난다.
제어 루프에서 클러스터 노드 개수를 주시하다 새 노드가 추가되면 해당 노드에 레플리카를 실행한다. 제어 루프는 파드 수도 주시하면서 파드가 제거되면 대체 파드를 실행한다.

```yaml
# 데몬셋의 template 필드 속 파드 정의
    spec:
      containers:
...		
      nodeSelector: # 특정 노드에서만 파드 실행
        kiamol: ch06 # kiamol=ch06 레이블이 부여된 노드만 대상
```
데몬셋을 삭제했다 다시 생성하면 기존 삭제한 데몬셋이 관리하던 파드는 새로 생성된 데몬셋이 관리하게 된다.
## k8s 객체 간 오너십
컨트롤러 리소스를 삭제하면 관리 대상 리소스는 잠시 남아있다 삭제된다.
이런 사라진 객체를 찾아 제거하는 가비지 컬렉터가 있다.
객체 간 오너십은 일종의 위계를 형성하게 된다.
쿠버네티스 리소스는 오직 레이블 셀렉터만을 이용해 의존 관계를 정의하기 때문에, 레이블을 멋대로 수정하면 의존 관계가 깨질 수 있다.
